{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c2cd6e-4b85-46d0-a585-96b04e455045",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a4a86-41a7-4865-8477-321f5dd5a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\t\n",
    "\t\n",
    "def preprocessing(file_name, nan_cnt, empty_policy, show_plot = True):\n",
    "\n",
    "    print(f\"filename:{file_name}\\nnan_cnt:{nan_cnt}\\nempty_policy:{empty_policy}\\n\")\n",
    "    \n",
    "    # load csv file\n",
    "    df_data = pd.read_csv(file_name)\n",
    "\n",
    "    # Feature engineering\n",
    "    isna_dict = dict(df_data.isna().sum())\n",
    "    isna_keys = list(isna_dict.keys())\n",
    "    del_column_idx = []\n",
    "    del_column_names = []\n",
    "    for idx, cnt in enumerate(isna_dict.values()):\n",
    "        if cnt >= nan_cnt:\n",
    "            #print(idx, cnt, isna_keys[idx])\n",
    "            del_column_idx.append(idx)\n",
    "            del_column_names.append(isna_keys[idx])\n",
    "\t\t\t\n",
    "    df_data = df_data.drop(del_column_names, axis=1)\n",
    "    df_data.describe()\n",
    "    \n",
    "    # conver dataframe to nparray\n",
    "    ips_data = np.array(df_data)\n",
    "\n",
    "\n",
    "    # calcuate min, avg for each wifi\n",
    "\n",
    "    wifi_ap_rssis_dict = {}\n",
    "    for row in ips_data:\n",
    "        for idx, rssi in enumerate(row[4:]):\n",
    "            mac = df_data.columns[4+idx]\n",
    "            if mac not in wifi_ap_rssis_dict.keys():\n",
    "                wifi_ap_rssis_dict[mac] = {'values':[]}\n",
    "            if math.isnan(rssi) is False:\n",
    "                wifi_ap_rssis_dict[mac]['values'].append(rssi)\n",
    "\n",
    "\t# wifi_ap_rssis_dict = { mac: {\"values\":[]}}\n",
    "\t# k is mac\n",
    "\t# v is {'values'}\n",
    "\t\t\t   \n",
    "    for v in wifi_ap_rssis_dict.values():\n",
    "        v['avg'] = sum(v['values'])/len(v['values'])\n",
    "        v['min'] = min(v['values'])\n",
    "        v['default'] = -120    \n",
    "\n",
    "\n",
    "    # fill empty data\n",
    "\n",
    "    if empty_policy not in ['min', 'avg', 'default']:\n",
    "        empty_policy = 'default'\n",
    "\n",
    "\n",
    "    for row in ips_data:\n",
    "        for idx in range(len(row[4:])):\n",
    "            col = 4+idx\n",
    "            mac = df_data.columns[col]\n",
    "            rssi = row[col]\n",
    "            if math.isnan(rssi) == True:\n",
    "                row[col] = wifi_ap_rssis_dict[mac][empty_policy]\n",
    "\n",
    "    \n",
    "    return ips_data[:, 4:], ips_data[:, 1].reshape(-1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609b7ce-8413-4754-92c0-09ba1f82516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = preprocessing(file_name=\"trace.csv\", nan_cnt=60, empty_policy='default')\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd069cd-7f22-4eb2-8f81-f6526c69ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ee159-0de7-4910-83c0-71a41380867c",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "> 텐서(tensor)는 배열(array)이나 행렬(matrix)과 매우 유사한 특수한 자료구조입니다.   \n",
    "PyTorch에서는 텐서를 사용하여 모델의 입력과 출력뿐만 아니라 모델의 매개변수를 부호화(encode)합니다.  \n",
    ">\n",
    "> GPU나 다른 연산 가속을 위한 특수한 하드웨어에서 실행할 수 있다는 점을 제외하면,  \n",
    "텐서는 NumPy의 ndarray와 매우 유사합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca4bef-ff3d-44c1-a330-cdbdf91bb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd86f1-ad86-4ee5-a65e-75520a2c257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "x = torch.from_numpy(x).float()\n",
    "y = torch.from_numpy(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88eef0-0f63-4b4d-998c-4c332d1d5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0dd04-9da0-473c-9b1d-0d7af357e2f7",
   "metadata": {},
   "source": [
    "# one-hot\n",
    "\n",
    "> 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고,  \n",
    "표현하고 싶은 단어의 인덱스에 1의 값을 부여하고,  \n",
    "다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다.   \n",
    "이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc04c0-eb14-4385-97cc-ac498ac6fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = F.one_hot(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe311249-e97a-4b2e-8c91-93b9bb8990e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625539f8-25f1-4e86-a9b9-f984599bd570",
   "metadata": {},
   "source": [
    "# Multi Linear Perceptron by Pytorch\n",
    "\n",
    "`nn.Model` 모든 신경망 모듈의 기본이 되는 클래스\n",
    "* hidden layer 에 따른 성능: https://towardsdatascience.com/from-animation-to-intuition-visualizing-optimization-trajectory-in-neural-nets-726e43a08d85https://towardsdatascience.com/from-animation-to-intuition-visualizing-optimization-trajectory-in-neural-nets-726e43a08d85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e50c9-ff89-47bc-a839-460484188fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "        \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),    # layer1 - input\n",
    "            nn.ReLU(),              \n",
    "            nn.BatchNorm1d(128), \n",
    "            \n",
    "            nn.Linear(128, 256),            # layer2 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 256),            # layer3 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 128),            # layer4 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Linear(128, out_features),    # layer5 - out\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.nn(x)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce719d2-2c43-412c-af18-d953c7c3deb4",
   "metadata": {},
   "source": [
    "# MLP 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430f125-216f-48f4-a3f0-11218008f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(25, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d7a75-b153-4dab-b40c-6ad1bb97788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU를 사용하기 위향 tensor를 GPU에 업로드\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "model = model.to(device)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "y_onehot = y_onehot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e398e6d-96d8-47da-a649-f4989bd58459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 크기 체크\n",
    "\n",
    "#!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(25,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e9309-936c-4928-9b77-1d073fa95c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6179e88d-4cfb-455d-a15e-8c7f04f113e8",
   "metadata": {},
   "source": [
    "# 학습 전 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffe1f7-528c-4384-bfb8-c13ced88b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(x[0].view(1,-1))    # MLP 네트워크의 output\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31e453-b324-46bd-ad6f-efea77f8576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 중 가장 값이 큰 class의 index 구하기\n",
    "confidence, predicted = torch.max(outputs, dim=1)\n",
    "print(confidence)        # 가장 큰 값\n",
    "print(predicted)         # 가장 큰 값을 가지는 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b7cdb-88b7-4b76-9a20-2fd97f1579a3",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a3725-8765-4e8b-aa84-d72e5193f5ae",
   "metadata": {},
   "source": [
    "# train/validation dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b2e94-2956-4931-830f-aba5d887a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:72]\n",
    "x_test = x[72:]\n",
    "y_train = y_onehot[:72]\n",
    "y_test = y_onehot[72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6a5be-c5ae-4a17-b33c-7252751e76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train[:5])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2f41a-3821-4bf9-ba2e-37fccb624540",
   "metadata": {},
   "source": [
    "# loss function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9437ea7-f49b-4a22-96bf-a1d41fd17653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60380d3-6fc6-4d61-98e4-24cd6edba17c",
   "metadata": {},
   "source": [
    "# train 을 위한 함수 정의\n",
    "\n",
    "## Optimizer 정의\n",
    "https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam\n",
    "\n",
    "## epoch\n",
    "\n",
    "> 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함.  \n",
    "> (즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태) \n",
    "> \n",
    "> 신경망에서 사용되는 역전파 알고리즘(backpropagation algorithm)은  \n",
    "> 파라미터를 사용하여 입력부터 출력까지의\n",
    "> 각 계층의 weight를 계산하는 과정을 거치는 순방향 패스(forward pass),  \n",
    "> forward pass를 반대로 거슬러 올라가며 다시 한 번 계산 과정을 거처 기존의 weight를 수정하는 역방향 패스(backward pass)로 나뉩니다. 이 전체 데이터 셋에 대해 해당 과정(forward pass + backward pass)이 완료되면 한 번의 epoch가 진행됐다고 볼 수 있습니다.\n",
    "\n",
    "* 출처: https://m.blog.naver.com/qbxlvnf11/221449297033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc123b-2ce0-458c-a583-f36a670062f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, criterion, lr=0.001, num_epoch = 10):\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # 그래프를 그리기 위한 loss값 저장\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    # optimizer 정의\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)    \n",
    "\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # training \n",
    "        \n",
    "        model.train()\n",
    "        outputs = model(x_train)\n",
    "        train_loss = criterion(outputs, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()     # 이전 epoch에서의 grad값 지우기\n",
    "        train_loss.backward()     # backward propogation : grad값 계산\n",
    "        optimizer.step()          # weight update\n",
    "        \n",
    "        \n",
    "        # for overfitting check\n",
    "        \n",
    "        model.eval()              # model을 evaluation 모드로 바꾸기 (grad 하지 않기)\n",
    "        \n",
    "        outputs = model(x_test)    # MLP 네트워크의 output\n",
    "        test_loss = criterion(outputs, y_test)\n",
    "        \n",
    "\n",
    "        # logging\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        skip = int(num_epoch/10)\n",
    "        if epoch%skip == 0:\n",
    "            print(f\"[{epoch:04d}], train_loss:{train_loss.item():.10f}, \"\\\n",
    "                  f\"test_loss:{test_loss.item():.10f} duration: {duration*skip:.3}s\")\n",
    "            \n",
    "        train_loss_list.append(train_loss.item())\n",
    "        test_loss_list.append(test_loss.item())\n",
    "    \n",
    "    return model, train_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c786b1e-d65e-4eaa-a329-d5015d21e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(train_losses, test_losses):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs = [i for i in range(1, len(train_losses)+1)]\n",
    "    plt.plot(epochs, train_losses, c=\"blue\", label=\"train loss\")\n",
    "    plt.plot(epochs, test_losses, c=\"red\", label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bf7d0-02df-4dfc-b3bc-64fa3ab9b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, test_losses = train_model(model, x_train, y_train, x_test, y_test, criterion,  num_epoch=100)\n",
    "draw_loss(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24e903-90a9-47ad-bf4e-048d6ab8426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, show_progress=True):\n",
    "    \n",
    "    hit_matrix = np.zeros((5,5))\n",
    "    \n",
    "    model.eval()\n",
    "    hit = 0\n",
    "    for i in range(len(x_test)):\n",
    "        outputs = model(x_test[i].view(1,-1))    # MLP 네트워크의 output\n",
    "        confidence, predicted = torch.max(outputs, 1)\n",
    "        #print(y_test[i])\n",
    "        _, actual = torch.max(y_test[i].view(1,-1), 1)\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"Predicted: {predicted.item()} ({confidence.item():.2f}), \"\\\n",
    "                  f\"actual: {actual.item()} / {predicted.item()==actual.item()}\")\n",
    "\n",
    "        if predicted[0].item() == actual[0].item():\n",
    "            hit += 1\n",
    "\n",
    "        hit_matrix[predicted.item()][actual.item()] += 1\n",
    "\n",
    "    print(f\"accuray: {hit/len(x_test)*100:.2f}% ({hit}/{len(x_test)})\")\n",
    "    \n",
    "    return hit_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262cf63-fd47-4b30-acf4-db72e4c7d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_matrix = evaluate(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f2ae0-51a8-4920-a7c0-e996ec704a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c356f-48d9-41f3-92c4-89684c13d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(hit_matrix):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "\n",
    "    map_df = pd.DataFrame(hit_matrix.astype(int))\n",
    "    map_df = map_df.iloc[::-1]  # row 순서 바꾸기\n",
    "    # heatmap\n",
    "\n",
    "    ax = plt.axes()\n",
    "\n",
    "    ax = sns.heatmap(map_df,\n",
    "                     ax=ax,\n",
    "                     cmap=\"Blues\",           # cmap Color\n",
    "                     annot=True,            # Value Text\n",
    "                     fmt=\"d\",             # Value type (interge = \"d\")\n",
    "                     linewidths=2)  \n",
    "    plt.xlabel('Predict', fontsize=14)\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eeee6f-8734-47a7-a446-cde7bf896b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmap(hit_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678c268-a148-4737-9dd8-02cc86a772eb",
   "metadata": {},
   "source": [
    "# 모델 생성 및 시뮬레이션을 위하여 필요한 부분만 발췌\n",
    "\n",
    "- learning rate를 0.1, 0.01, 0.001 로 변경해가면서 실험\n",
    "- 모델의 레이어를 변경해가면서 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce78be8-8ef9-490f-9a9b-f8aced8844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "        \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),    # layer1 - input\n",
    "            nn.ReLU(),              \n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.Linear(128, 256),            # layer2 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 256),            # layer3 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            #nn.Linear(256, 256),            # layer block - hidden\n",
    "            #nn.ReLU(),\n",
    "            #nn.BatchNorm1d(256),\n",
    "\n",
    "            #nn.Linear(256, 256),            # layer block - hidden\n",
    "            #nn.ReLU(),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            \n",
    "            #nn.Linear(256, 256),            # layer block - hidden\n",
    "            #nn.ReLU(),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 128),            # layer4 - hidden\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, out_features),    # layer5 - out\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.nn(x)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81392fe-5a23-4e68-8931-988302f49e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(25, 5).to(device)\n",
    "acc_train_loss = []\n",
    "acc_test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4a479-aa34-408d-b494-042b0cb51f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, train_loss, test_loss = train_model(model, x_train, y_train, x_test, y_test, criterion, lr=0.1, num_epoch=1000)\n",
    "\n",
    "acc_train_loss.extend(train_loss)\n",
    "acc_test_loss.extend(test_loss)\n",
    "#draw_loss(train_loss, test_loss)\n",
    "draw_loss(acc_train_loss, acc_test_loss)\n",
    "\n",
    "hit_matrix = evaluate(model, x_test, y_test, show_progress=False)\n",
    "draw_heatmap(hit_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
