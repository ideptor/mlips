{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e2033d-923e-4cd0-89ae-75887e976088",
   "metadata": {},
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f3cbd-df2c-4a4e-a9f3-aa4553af3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_train as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7367d37-0217-4be1-8148-d837a6483a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67f599-b71f-4b5c-b3e5-efe219292e01",
   "metadata": {},
   "source": [
    "# GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6731c-7708-49a3-960c-f4c85fd91403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916d001-809e-42bb-ae24-f1c315c67ceb",
   "metadata": {},
   "source": [
    "# Download MNIST dataset\n",
    "\n",
    "## What is MNIST dataset?\n",
    "> MNIST 데이터베이스 (Modified National Institute of Standards and Technology database)는  \n",
    "손으로 쓴 숫자들로 이루어진 대형 데이터베이스이며,   \n",
    "다양한 화상 처리 시스템을 트레이닝하기 위해 일반적으로 사용된다.   \n",
    "이 데이터베이스는 또한 기계 학습 분야의 트레이닝 및 테스트에 널리 사용된다.  \n",
    ">  \n",
    "> https://ko.wikipedia.org/wiki/MNIST_데이터베이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139dcb5-e7f8-4673-ad13-d30fd4186ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3bcea-8819-4f8c-b1b3-b3057084caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 특성\n",
    "print(train_data)\n",
    "print(train_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df13561-ce34-45fd-af60-cfc92f095d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 어떤 내용이 있는지 확인\n",
    "img, label = train_data[0]\n",
    "print(img.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecedd24-a87b-4c1d-b799-5cc29555a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.plot_train_data(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5082700-1d3b-422a-b87c-3ef5a42cc2e2",
   "metadata": {},
   "source": [
    "# Plot multiple train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682af98-01d8-49fa-8db9-aed5fe8ce9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.plot_multiple_train_data(train_data, rows=5, cols=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be9495-e9ad-4e95-98f7-eb7c8394c095",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "설정된 `batch_size` 단위로 데이터를 가져올 수 있도록 하는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf13b84-bbf5-45d2-8cdc-d232b729f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, ),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14afa6c-c3b6-4f18-82c8-88e9e7f2d282",
   "metadata": {},
   "source": [
    "# Defile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424a7ed-45a5-4064-883c-f52b752fe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(                        # 28*28 -> 24*24  \n",
    "                in_channels=1,              \n",
    "                out_channels=32,            \n",
    "                kernel_size=5,                      \n",
    "                stride=1,                   \n",
    "                padding=0,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),     # 24*24 -> 12*12\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(32, 64, 5, 1, 0),      # 12*12 -> 8*8\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                 # 8*8 -> 4*4\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(64 * 4 * 4, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d3e0c-1987-4e63-b082-5e31d3d84bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd1095-8e92-4dca-91de-0b74d06ae1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d7822-e995-4acb-9775-5e18688ce080",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.show_sample_predict_cnn(model, device, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d46da9-da0b-4851-83b3-e3c6b540d8dd",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f66efd-83e4-4a4b-8027-8a02224dc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(model, loaders, num_epochs, loss_func, optimizer, train_loss_list:list, test_loss_list:list):\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_dict = {\n",
    "            'train': 0.,\n",
    "            'test': 0.\n",
    "        }        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for phase in ['train', 'test']:\n",
    "        #for phase in ['train']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            for i, (images, labels) in enumerate(loaders[phase]):\n",
    "\n",
    "                images = images.to(device, dtype=torch.float32)\n",
    "                labels = labels.to(device)\n",
    "                # gives batch data, normalize x when iterate train_loader\n",
    "                b_x = Variable(images)   # batch x\n",
    "                b_y = Variable(labels)   # batch y\n",
    "\n",
    "                output = model(b_x)             \n",
    "\n",
    "                loss = loss_func(output, b_y)\n",
    "                loss_dict[phase] += loss.item()\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()     # clear gradients for this training step               \n",
    "                    loss.backward()           # backpropagation, compute gradients         \n",
    "                    optimizer.step()          # apply gradients                 \n",
    "\n",
    "                    # batch 100번 마다 로그 찍기\n",
    "                    if (i+1) % 100 == 0:\n",
    "                        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.6f}' \n",
    "                               .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    \n",
    "        train_loss = loss_dict['train'] / len(loaders['train']) \n",
    "        test_loss = loss_dict['test'] / len(loaders['test']) \n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] summary, train_loss:{train_loss:8.8f}, \"\\\n",
    "              f\"test_loss:{test_loss:8.8f} duration: {duration:.1f}s\")\n",
    "\n",
    "    return model, train_loss_list, test_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675d8b6-3640-421d-a09f-e6b41ac128fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)   \n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8466a-379b-4fee-8b8f-f9aee006752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss_list, test_loss_list = train(model, loaders, 10, loss_func, optimizer, train_loss_list, test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb18e4-5f32-4c63-bb87-cbe715e77cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.draw_loss(train_loss_list, test_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc606c-43f5-4241-9915-cebc0a786d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2b62a-fdd3-4a66-8199-69a7b325c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            pass\n",
    "\n",
    "        print(f'Test Accuracy of the model on the {len(test_data)} test images: {accuracy*100:.2f}%')\n",
    "    \n",
    "    pass\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d7002-3c49-4593-aefd-aeb86e6f2fec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64225a-d991-439b-91fc-ad4593b6c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    model.eval()\n",
    "    sample_idx = np.random.randint(len(test_data), size=(1,)).item()\n",
    "    img, gt = test_data[sample_idx]\n",
    "    img = img.to(device)\n",
    "    predicted = model(img.view(1,1,28,28))\n",
    "    #label = torch.argmax(predicted)\n",
    "    confidence, label = torch.max(predicted, 1)\n",
    "    label = label.cpu().item()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"{label}/(GT:{gt} / {gt==label})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.view(28,28).cpu(), cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd981c5-661e-4e88-a821-8d9bf86de54f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811c9fe-098a-4ada-b065-f7b58cd4f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.show_sample_predict_cnn(model, device, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bd3b8-79bb-4fa6-9e27-f82f80d7a6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc366f5-db8f-4726-be22-3f03ad99f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
